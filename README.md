# SummAI NLP Challenge

The objective of this project is to fine-tune the LLaMA-2 model using a few-shot prompting strategy. 

## Technical Prerequisites

Given the computational constraints, I utilized Google Colab with a T4 GPU.

Below are the versions of libraries that I used:


## Challenges and Limitations

During the course of implementation, several challenges were encountered that hindered the full exploration and experimentation of additional ideas from the literature.  But I found the approaches described in the following papers quite promising for the defined objective:

- Wang, Han & Prakash, Nirmalendu & Hoang, Nguyen & Hee, Ming Shan & Naseem, Usman & Lee, Roy Ka-Wei. (2023). Prompting Large Language Models for Topic Modeling.
- Houlsby, Neil & Giurgiu, Andrei & Jastrzebski, Stanislaw & Morrone, Bruna & Laroussilhe, Quentin & Gesmundo, Andrea & Attariyan, Mona & Gelly, Sylvain. (2019). Parameter-Efficient Transfer Learning for NLP.
and in particular
- Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., ... & Chen, W. (2021). Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685.
