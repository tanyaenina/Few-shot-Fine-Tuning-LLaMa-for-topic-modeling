{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#install required libraries if needed\n",
        "!pip3 install transformers==4.33.2\n",
        "!pip3 install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/"
      ],
      "metadata": {
        "id": "ZVBAf5ZQpWbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3DcvTe7dnz7j"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name_or_path = 'TheBloke/Llama-2-13B-chat-GPTQ'\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name_or_path, revision='gptq-4bit-32g-actorder_True',device_map='auto',trust_remote_code=False)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)"
      ],
      "metadata": {
        "id": "sENqX9bKpSiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define the device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "eOqNxQOttQMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_inst, end_inst = '[INST]', '[/INST]'\n",
        "start_sys, end_sys = '<<SYS>>\\n', '\\n<</SYS>>\\n\\n'\n",
        "\n",
        "sys_prompt = \"\"\"\\\n",
        "You are designated as an assistant that identify and extract high-level topics from articles. \\\n",
        "You should avoid giving specific details and provide unique topics solely. Each topic should be represented as a single word\\\\\n",
        "\"\"\"\n",
        "#return the concatenation of system prompt and instruction (with corresponding start and end tokens)\n",
        "def create_prompt(instruction, system_prompt, start_inst=start_inst, end_inst=end_inst, start_sys=start_sys, end_sys=end_sys):\n",
        "    prompt = start_inst+start_sys+system_prompt+end_sys+instruction+end_inst\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "s-WTTaUutYE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remove the input prompt from the output\n",
        "def format_output(raw_out, prompt, tokenizer):\n",
        "    out = tokenizer.decode(raw_out[0],skip_special_tokens=True)\n",
        "    out_new = out.replace(prompt,' ')\n",
        "    final_out = out_new.strip()\n",
        "    return final_out"
      ],
      "metadata": {
        "id": "-rxUElgqvqOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#generate prompt\n",
        "def generate(instruction, system_prompt, model=model, tokenizer=tokenizer, device=device):\n",
        "    prompt = create_prompt(instruction,system_prompt)\n",
        "    encoded_input = tokenizer(prompt, return_tensors=\"pt\").to(device=model.device)\n",
        "    output = model.generate(input_ids = encoded_input['input_ids'], do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512, temperature=0.7,)\n",
        "    f = format_output(output,prompt,tokenizer)\n",
        "    return f"
      ],
      "metadata": {
        "id": "uvLosgIRwkzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#it as an example of instruction with 4 provided demonstrations of sentences with topics and a short text for which the model has to define a topic.\n",
        "inst = \"Political parties play a crucial role in modern democracies, serving as a bridge between the government and the public. // Politics \\\n",
        "The economy plays a crucial role in shaping the prosperity and stability of a nation, influencing everything from employment rates to the standard of living. //Economy \\\n",
        "Vegetables are an essential component of a healthy diet, providing a wide range of nutrients that are vital for maintaining good health. // Vegetables \\\n",
        "Football foster teamwork, discipline, and physical fitness, bringing people together across cultures and creating a sense of community and excitement. // Sports \\\n",
        "Fruits are a delicious and nutritious part of a healthy diet, packed with essential vitamins, minerals, and antioxidants that support overall health and well-being. // \"\n",
        "\n",
        "result = generate(inst,sys_prompt)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyktSs-RwuYU",
        "outputId": "2bdc2763-b9f3-4269-a2b5-480e56fc57f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sure, I can help you with that! Here are the high-level topics I've identified from the given text, represented as single words:\n",
            "\n",
            "1. Politics\n",
            "2. Economy\n",
            "3. Vegetables\n",
            "4. Sports\n",
            "5. Fruits\n",
            "\n",
            "I've avoided giving specific details and focused on the main themes and topics present in the text.\n"
          ]
        }
      ]
    }
  ]
}